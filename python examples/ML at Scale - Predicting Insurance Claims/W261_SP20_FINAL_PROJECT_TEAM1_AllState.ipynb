{"cells":[{"cell_type":"markdown","source":["# Final Project:  Predicting Allstate Claim Loss Value\n\n### W261 / Spring 2020 / Team One \n\n#### Alex West, Sarah Danzi, John Boudreaux\n\nData for this report was provided by Kaggle in the Allstate Claims competition: https://www.kaggle.com/c/allstate-claims-severity"],"metadata":{}},{"cell_type":"code","source":["# Import packages used in this notebook.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nimport seaborn as sns\n\nfrom pyspark.ml.linalg import Vectors, DenseMatrix\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\nfrom pyspark.ml.feature import PCA as ps_PCA\nfrom pyspark.ml.regression import LinearRegression as ps_linreg\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import RandomForestRegressor as ps_rf\n\nfrom collections import defaultdict\nfrom sklearn.pipeline import Pipeline as skPipeline\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import linear_model\nsns.set_style(\"whitegrid\")\n\n# Variable Definitions\nunpacked_data_dir  = 'dbfs:/user/john.boudreaux@ischool.berkeley.edu/FINAL_PROJECT/'   # Directory where data exists\nrandom_seed = 2020                                                                     # Random seed "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Read the training data into a Spark dataframe\ns_df = spark.read.format('csv').options(header='true', inferSchema='true').load(unpacked_data_dir+\"train.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Question Formulations"],"metadata":{}},{"cell_type":"markdown","source":["The cost of an insurance claim may be measured in dollars, but carries a much more significant impact to those affected. An automated method of predicting the cost - or severity - of a claim could save time, money, and distress for both the insurer and the insured. To explore potential methods, Allstate released a competition on Kaggle to create an algorithm to accurately predict the cost of a claim. The purpose of this competition was twofold for Allstate: one, to gain access to innovative ideas, and two, to recruit talented individuals. \n\nFor the purposes of this project, our goals are slightly different. While we are interested in the performance of our models, we are also interested in exploring the different systems available to accomplish the task. The data from the Kaggle competition are neatly packaged and big enough to train a model, but not big enough to require distributed computing. In the real world, however, data are never so perfect, and especially for a large company like Allstate, so small. Therefore, through the course of this project we are using the Allstate data and Kaggle competition objective to explore the difference between two data science tools: **scikit-learn** (or sklearn) for smaller non-distributed data, and **Spark** for large distributed data sets. \n\nThe data remain the same, with attribute columns pertaining to properties of a claim filed with Allstate. The outcome variable `loss` is a dollar amount requested by the insured to the insurer. This analysis is critical to the insurance industry; accurately predicting a claim value would allow the company to allocate resources effectively, and potentially predict resource needs at a grander scale. It could also allow a company to predict the total claim value of a customer over the course of their engagement with the company, and evaluate risk. \n\nA useful model would need to be able to predict the value of a claim within a predetermined order of magnitude. For the purposes of the competition that order of magnitude could be fairly high, say $1,000. In reality, multiplied over millions of customers, this performance may not be adequate. With this in mind, we are inclined to pay close attention to the distribution of the data (in particular the outcome variable `loss`) and perform transformations where appropriate.\n\nTo measure the success of our model, we will use Mean Absolute Error (or MAE). This is a prediction problem, not classification, and therefore other metrics of success (accuracy, recall, precision, etc) are meaningless unless we change the structure of the data. We care about the distance from the actual amount, and therefore could use either Mean Absolute Error `MAE = mean(abs(y - y_pred))` or Mean Squared Error `MSE = mean((y - y_pred)^2)`. Both MAE and MSE express average model prediction error in units of the variable of interest, but MSE penalizes large errors more (so if being off by 10 is more than twice as bad as being off by 5, use MSE). In this case, we don't have any indication that error severity increases with dollar amount, and we have another big advantage with MAE: interpretability. And finally, it is the same metric used by the Kaggle competition so we will be able to compare our results to a larger group.\n\nOur notebook will first start with exploratory data analysis, then move to feature engineering and algorithm exploration, discuss performance with algorithm implementation, and conclude with a discussion of the capabilities of Spark and sklearn."],"metadata":{}},{"cell_type":"markdown","source":["## Exploratory Data Analysis & Discussion of Challenges \n\nIdentifying an approach for predicting Allstate claim loss begins by examining the pool of available data.  This exploratory data analysis activity has several overarching goals:\n\n  * Understand the scale, shape, and distribution of the data.\n  * Preliminarily identify features for use.  \n  * Store the data, inclusive of any transformations/corrections, for further analysis and scalable processing.\n\nThis report addresses each of these steps in sequential order with inline code and visualizations to fully characterize the dataset.  \n\n#### Scale, Shape, & Distribution of Data\n\nWe perform our exploratory data analysis on the `train.csv` file provided by Allstate.  The file is 0.065 GB and contains 188,318 rows and 132 columns. Of the 132 columns, 130 are features that describe each observation, or row.  The two remaining columns are `id`, a unique identifier for each observation, and `loss`, identified by the dataset's metadata as the outcome variable. \n\nOur first observation is thus that the data set is quite small by big data standards.  This, in theory, would allow us to explore and develop predictive models using non-parallelized computing options.  However, we recognize that given the source of the dataset, Kaggle, this training dataset may represent only a subset of the company's actual data.  Thus, we will want to pursue developing both a parallelized ML solution and a non-parallelized ML solution to offer options to the business.  Doing this will also allow us to compare and contrast the solutions to develop statements on efficiency and resource utilization for our Customer."],"metadata":{}},{"cell_type":"code","source":["# Print training filename and size\nfor item in dbutils.fs.ls(unpacked_data_dir + \"train.csv\"):\n  print(\"*  File:\", item.name)\n  print(\"   Size:\", item.size / 2**30, \"GB\")\n\n# Print shape of the data \nprint(\"\\n* Shape:\\n   Rows:\", s_df.count(), \" Columns:\", len(s_df.columns))\n\n# Print dataset schema\nprint(\"\\n*  Dataset Schema:\")\nprint(s_df.printSchema())\n\n# Preview the values of one row (e.g., one datapoint)\nprint(\"\\n*  Row One:\\n\", s_df.head())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">*  File: train.csv\n   Size: 0.06521617900580168 GB\n\n* Shape:\n   Rows: 188318  Columns: 132\n\n*  Dataset Schema:\nroot\n-- id: integer (nullable = true)\n-- cat1: string (nullable = true)\n-- cat2: string (nullable = true)\n-- cat3: string (nullable = true)\n-- cat4: string (nullable = true)\n-- cat5: string (nullable = true)\n-- cat6: string (nullable = true)\n-- cat7: string (nullable = true)\n-- cat8: string (nullable = true)\n-- cat9: string (nullable = true)\n-- cat10: string (nullable = true)\n-- cat11: string (nullable = true)\n-- cat12: string (nullable = true)\n-- cat13: string (nullable = true)\n-- cat14: string (nullable = true)\n-- cat15: string (nullable = true)\n-- cat16: string (nullable = true)\n-- cat17: string (nullable = true)\n-- cat18: string (nullable = true)\n-- cat19: string (nullable = true)\n-- cat20: string (nullable = true)\n-- cat21: string (nullable = true)\n-- cat22: string (nullable = true)\n-- cat23: string (nullable = true)\n-- cat24: string (nullable = true)\n-- cat25: string (nullable = true)\n-- cat26: string (nullable = true)\n-- cat27: string (nullable = true)\n-- cat28: string (nullable = true)\n-- cat29: string (nullable = true)\n-- cat30: string (nullable = true)\n-- cat31: string (nullable = true)\n-- cat32: string (nullable = true)\n-- cat33: string (nullable = true)\n-- cat34: string (nullable = true)\n-- cat35: string (nullable = true)\n-- cat36: string (nullable = true)\n-- cat37: string (nullable = true)\n-- cat38: string (nullable = true)\n-- cat39: string (nullable = true)\n-- cat40: string (nullable = true)\n-- cat41: string (nullable = true)\n-- cat42: string (nullable = true)\n-- cat43: string (nullable = true)\n-- cat44: string (nullable = true)\n-- cat45: string (nullable = true)\n-- cat46: string (nullable = true)\n-- cat47: string (nullable = true)\n-- cat48: string (nullable = true)\n-- cat49: string (nullable = true)\n-- cat50: string (nullable = true)\n-- cat51: string (nullable = true)\n-- cat52: string (nullable = true)\n-- cat53: string (nullable = true)\n-- cat54: string (nullable = true)\n-- cat55: string (nullable = true)\n-- cat56: string (nullable = true)\n-- cat57: string (nullable = true)\n-- cat58: string (nullable = true)\n-- cat59: string (nullable = true)\n-- cat60: string (nullable = true)\n-- cat61: string (nullable = true)\n-- cat62: string (nullable = true)\n-- cat63: string (nullable = true)\n-- cat64: string (nullable = true)\n-- cat65: string (nullable = true)\n-- cat66: string (nullable = true)\n-- cat67: string (nullable = true)\n-- cat68: string (nullable = true)\n-- cat69: string (nullable = true)\n-- cat70: string (nullable = true)\n-- cat71: string (nullable = true)\n-- cat72: string (nullable = true)\n-- cat73: string (nullable = true)\n-- cat74: string (nullable = true)\n-- cat75: string (nullable = true)\n-- cat76: string (nullable = true)\n-- cat77: string (nullable = true)\n-- cat78: string (nullable = true)\n-- cat79: string (nullable = true)\n-- cat80: string (nullable = true)\n-- cat81: string (nullable = true)\n-- cat82: string (nullable = true)\n-- cat83: string (nullable = true)\n-- cat84: string (nullable = true)\n-- cat85: string (nullable = true)\n-- cat86: string (nullable = true)\n-- cat87: string (nullable = true)\n-- cat88: string (nullable = true)\n-- cat89: string (nullable = true)\n-- cat90: string (nullable = true)\n-- cat91: string (nullable = true)\n-- cat92: string (nullable = true)\n-- cat93: string (nullable = true)\n-- cat94: string (nullable = true)\n-- cat95: string (nullable = true)\n-- cat96: string (nullable = true)\n-- cat97: string (nullable = true)\n-- cat98: string (nullable = true)\n-- cat99: string (nullable = true)\n-- cat100: string (nullable = true)\n-- cat101: string (nullable = true)\n-- cat102: string (nullable = true)\n-- cat103: string (nullable = true)\n-- cat104: string (nullable = true)\n-- cat105: string (nullable = true)\n-- cat106: string (nullable = true)\n-- cat107: string (nullable = true)\n-- cat108: string (nullable = true)\n-- cat109: string (nullable = true)\n-- cat110: string (nullable = true)\n-- cat111: string (nullable = true)\n-- cat112: string (nullable = true)\n-- cat113: string (nullable = true)\n-- cat114: string (nullable = true)\n-- cat115: string (nullable = true)\n-- cat116: string (nullable = true)\n-- cont1: double (nullable = true)\n-- cont2: double (nullable = true)\n-- cont3: double (nullable = true)\n-- cont4: double (nullable = true)\n-- cont5: double (nullable = true)\n-- cont6: double (nullable = true)\n-- cont7: double (nullable = true)\n-- cont8: double (nullable = true)\n-- cont9: double (nullable = true)\n-- cont10: double (nullable = true)\n-- cont11: double (nullable = true)\n-- cont12: double (nullable = true)\n-- cont13: double (nullable = true)\n-- cont14: double (nullable = true)\n-- loss: double (nullable = true)\n\nNone\n\n*  Row One:\n Row(id=1, cat1=&#39;A&#39;, cat2=&#39;B&#39;, cat3=&#39;A&#39;, cat4=&#39;B&#39;, cat5=&#39;A&#39;, cat6=&#39;A&#39;, cat7=&#39;A&#39;, cat8=&#39;A&#39;, cat9=&#39;B&#39;, cat10=&#39;A&#39;, cat11=&#39;B&#39;, cat12=&#39;A&#39;, cat13=&#39;A&#39;, cat14=&#39;A&#39;, cat15=&#39;A&#39;, cat16=&#39;A&#39;, cat17=&#39;A&#39;, cat18=&#39;A&#39;, cat19=&#39;A&#39;, cat20=&#39;A&#39;, cat21=&#39;A&#39;, cat22=&#39;A&#39;, cat23=&#39;B&#39;, cat24=&#39;A&#39;, cat25=&#39;A&#39;, cat26=&#39;A&#39;, cat27=&#39;A&#39;, cat28=&#39;A&#39;, cat29=&#39;A&#39;, cat30=&#39;A&#39;, cat31=&#39;A&#39;, cat32=&#39;A&#39;, cat33=&#39;A&#39;, cat34=&#39;A&#39;, cat35=&#39;A&#39;, cat36=&#39;A&#39;, cat37=&#39;A&#39;, cat38=&#39;A&#39;, cat39=&#39;A&#39;, cat40=&#39;A&#39;, cat41=&#39;A&#39;, cat42=&#39;A&#39;, cat43=&#39;A&#39;, cat44=&#39;A&#39;, cat45=&#39;A&#39;, cat46=&#39;A&#39;, cat47=&#39;A&#39;, cat48=&#39;A&#39;, cat49=&#39;A&#39;, cat50=&#39;A&#39;, cat51=&#39;A&#39;, cat52=&#39;A&#39;, cat53=&#39;A&#39;, cat54=&#39;A&#39;, cat55=&#39;A&#39;, cat56=&#39;A&#39;, cat57=&#39;A&#39;, cat58=&#39;A&#39;, cat59=&#39;A&#39;, cat60=&#39;A&#39;, cat61=&#39;A&#39;, cat62=&#39;A&#39;, cat63=&#39;A&#39;, cat64=&#39;A&#39;, cat65=&#39;A&#39;, cat66=&#39;A&#39;, cat67=&#39;A&#39;, cat68=&#39;A&#39;, cat69=&#39;A&#39;, cat70=&#39;A&#39;, cat71=&#39;A&#39;, cat72=&#39;A&#39;, cat73=&#39;A&#39;, cat74=&#39;A&#39;, cat75=&#39;B&#39;, cat76=&#39;A&#39;, cat77=&#39;D&#39;, cat78=&#39;B&#39;, cat79=&#39;B&#39;, cat80=&#39;D&#39;, cat81=&#39;D&#39;, cat82=&#39;B&#39;, cat83=&#39;D&#39;, cat84=&#39;C&#39;, cat85=&#39;B&#39;, cat86=&#39;D&#39;, cat87=&#39;B&#39;, cat88=&#39;A&#39;, cat89=&#39;A&#39;, cat90=&#39;A&#39;, cat91=&#39;A&#39;, cat92=&#39;A&#39;, cat93=&#39;D&#39;, cat94=&#39;B&#39;, cat95=&#39;C&#39;, cat96=&#39;E&#39;, cat97=&#39;A&#39;, cat98=&#39;C&#39;, cat99=&#39;T&#39;, cat100=&#39;B&#39;, cat101=&#39;G&#39;, cat102=&#39;A&#39;, cat103=&#39;A&#39;, cat104=&#39;I&#39;, cat105=&#39;E&#39;, cat106=&#39;G&#39;, cat107=&#39;J&#39;, cat108=&#39;G&#39;, cat109=&#39;BU&#39;, cat110=&#39;BC&#39;, cat111=&#39;C&#39;, cat112=&#39;AS&#39;, cat113=&#39;S&#39;, cat114=&#39;A&#39;, cat115=&#39;O&#39;, cat116=&#39;LB&#39;, cont1=0.7263, cont2=0.245921, cont3=0.187583, cont4=0.789639, cont5=0.310061, cont6=0.718367, cont7=0.33506, cont8=0.3026, cont9=0.67135, cont10=0.8351, cont11=0.569745, cont12=0.594646, cont13=0.822493, cont14=0.714843, loss=2213.18)\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["We next test the dataset to identify if there are any missing values or obvious integrity issues.  Luckily, no such issues are identified:  each cell of the dataset is populated."],"metadata":{}},{"cell_type":"code","source":["# Determine if any values are missing\nprint(\"* Count the number of missing values in each column:\")\ns_df.select([F.count(F.when(F.isnan(c), c)).alias(c) for c in s_df.columns]).toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>cat1</th>\n      <th>cat2</th>\n      <th>cat3</th>\n      <th>cat4</th>\n      <th>cat5</th>\n      <th>cat6</th>\n      <th>cat7</th>\n      <th>cat8</th>\n      <th>cat9</th>\n      <th>cat10</th>\n      <th>cat11</th>\n      <th>cat12</th>\n      <th>cat13</th>\n      <th>cat14</th>\n      <th>cat15</th>\n      <th>cat16</th>\n      <th>cat17</th>\n      <th>cat18</th>\n      <th>cat19</th>\n      <th>cat20</th>\n      <th>cat21</th>\n      <th>cat22</th>\n      <th>cat23</th>\n      <th>cat24</th>\n      <th>cat25</th>\n      <th>cat26</th>\n      <th>cat27</th>\n      <th>cat28</th>\n      <th>cat29</th>\n      <th>cat30</th>\n      <th>cat31</th>\n      <th>cat32</th>\n      <th>cat33</th>\n      <th>cat34</th>\n      <th>cat35</th>\n      <th>cat36</th>\n      <th>cat37</th>\n      <th>cat38</th>\n      <th>cat39</th>\n      <th>...</th>\n      <th>cat92</th>\n      <th>cat93</th>\n      <th>cat94</th>\n      <th>cat95</th>\n      <th>cat96</th>\n      <th>cat97</th>\n      <th>cat98</th>\n      <th>cat99</th>\n      <th>cat100</th>\n      <th>cat101</th>\n      <th>cat102</th>\n      <th>cat103</th>\n      <th>cat104</th>\n      <th>cat105</th>\n      <th>cat106</th>\n      <th>cat107</th>\n      <th>cat108</th>\n      <th>cat109</th>\n      <th>cat110</th>\n      <th>cat111</th>\n      <th>cat112</th>\n      <th>cat113</th>\n      <th>cat114</th>\n      <th>cat115</th>\n      <th>cat116</th>\n      <th>cont1</th>\n      <th>cont2</th>\n      <th>cont3</th>\n      <th>cont4</th>\n      <th>cont5</th>\n      <th>cont6</th>\n      <th>cont7</th>\n      <th>cont8</th>\n      <th>cont9</th>\n      <th>cont10</th>\n      <th>cont11</th>\n      <th>cont12</th>\n      <th>cont13</th>\n      <th>cont14</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 132 columns</p>\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Of the 130 feature columns, Spark inferred upon read-in that 116 of the columns are of type `string`.  For the columns of type `string`, all are  named `catXXX`, where `XXX` is a unique numeric identifier.  Given this information, We presume these features to be categorical in nature and seek to confirm our belief.  The plot shown below quantifies the distribution of unique values that exist within each column.  Seventy-five percent of the columns have four unique values or less;  at least half of the columns have only two unique values.  This distribution corresponds with a belief that these columns are indeed categorical.  Given this distribution, we will likely want to strongly consider one-hot encoding emerges as part of our feature engineering effort.  \n\nSeveral columns, however, have a very high number of unique values:  for example, column `cat116` has 326 unique values and column `cat110` has 131.  Given that we do not have any interpretable meaning for these columns, we withhold making any assumptions of whether one-hot encoding categorical variables with such a large number of categories is of value.  However, we highlight their presence in the dataset during exploratory analysis as future investigation that weighs their predictive benefit to a solution may be warranted to increase execution times and system resource demands."],"metadata":{}},{"cell_type":"code","source":["# Identify the categorical column names\ncat_cols = [w for w in s_df.columns if 'cat' in w]\n\n# Identify the continuous valued column names\ncont_cols = [w for w in s_df.columns if 'cont' in w]\n\n# Filter dataframes into categorical features  and continuous features\ns_cat_df = s_df.select(*cat_cols)\ns_cont_df = s_df.select(*cont_cols)\n\n# Plot the distribution of categorical feature values\ncat_df = s_cat_df.toPandas()\nfig, ax = plt.subplots(figsize=(8,3))\nsns.distplot(cat_df.nunique(), bins = 100, kde=False, rug=False)\nax.set_title('Categorical Features')\nax.set_ylabel('Quantity')\nax.set_xlabel('Number of Unique Values in a Column')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["print(\"Statistical details of the distribution:\")\nprint(cat_df.nunique().describe())\n\nprint(\"\\nCategorical features with more than two categories:\", )\nprint(cat_df[cat_df.columns[np.where(cat_df.nunique() > 2)]].nunique())\n\nprint(\"\\nData for the categorical features plot:\")\nprint(cat_df.nunique().value_counts())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Statistical details of the distribution:\ncount    116.000000\nmean       9.818966\nstd       33.666807\nmin        2.000000\n25%        2.000000\n50%        2.000000\n75%        4.000000\nmax      326.000000\ndtype: float64\n\nCategorical features with more than two categories:\ncat73       3\ncat74       3\ncat75       3\ncat76       3\ncat77       4\ncat78       4\ncat79       4\ncat80       4\ncat81       4\ncat82       4\ncat83       4\ncat84       4\ncat85       4\ncat86       4\ncat87       4\ncat88       4\ncat89       8\ncat90       7\ncat91       8\ncat92       7\ncat93       5\ncat94       7\ncat95       5\ncat96       8\ncat97       7\ncat98       5\ncat99      16\ncat100     15\ncat101     19\ncat102      9\ncat103     13\ncat104     17\ncat105     20\ncat106     17\ncat107     20\ncat108     11\ncat109     84\ncat110    131\ncat111     16\ncat112     51\ncat113     61\ncat114     19\ncat115     23\ncat116    326\ndtype: int64\n\nData for the categorical features plot:\n2      72\n4      12\n3       4\n7       4\n5       3\n8       3\n20      2\n19      2\n17      2\n16      2\n11      1\n9       1\n84      1\n131     1\n326     1\n15      1\n23      1\n51      1\n61      1\n13      1\ndtype: int64\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Spark inferred 14 columns to be of type `double`, all of which are named `contXX`, where `XX` is a unique numeric identifier.  We thus presume these features to be continuous numeric data and plot each to examine its distribution of data.  The visualizations highlight a few interesting characteristics of these columns.  First, all of the plots show that the each column contains values that are all positive and bound between zero and one.  This is convenient for us as we will not need to take corrective action to either center or scale the data.  It is noteworthy, however, that many of our continuous features are not normally distributed.  This isn't necessarily a problem, but could have consequences downstream depending on our solution selection."],"metadata":{}},{"cell_type":"code","source":["# Plot the continuous-valued columns\nfig, axes = plt.subplots(5, 3, figsize=(12, 8), sharex=True)\nfor ax, feature in zip(axes.flat, cont_cols):\n    sns.distplot(s_df.select(feature).toPandas(), ax=ax, kde=False, rug=False)\n    ax.tick_params(labelsize=7)\n    ax.set_xlabel(feature, fontsize=9)\n\n# Blank out the last graph\naxes[4][2].set_xticks([])\naxes[4][2].set_yticks([])\n\nfig.suptitle('Continuous Feature Distributions', fontsize=12)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Examine the continuously valued data.\ns_cont_df.describe().show(truncate=False, vertical=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">-RECORD 0----------------------\n summary | count               \n cont1   | 188318              \n cont2   | 188318              \n cont3   | 188318              \n cont4   | 188318              \n cont5   | 188318              \n cont6   | 188318              \n cont7   | 188318              \n cont8   | 188318              \n cont9   | 188318              \n cont10  | 188318              \n cont11  | 188318              \n cont12  | 188318              \n cont13  | 188318              \n cont14  | 188318              \n-RECORD 1----------------------\n summary | mean                \n cont1   | 0.4938613645641969  \n cont2   | 0.5071883561794216  \n cont3   | 0.4989184507216435  \n cont4   | 0.49181230258924097 \n cont5   | 0.4874277287832548  \n cont6   | 0.49094453373549346 \n cont7   | 0.4849702050680203  \n cont8   | 0.4864373158699669  \n cont9   | 0.4855063198950639  \n cont10  | 0.4980658504232179  \n cont11  | 0.4935110085546642  \n cont12  | 0.4931504256258122  \n cont13  | 0.4931376158359727  \n cont14  | 0.4957170179749155  \n-RECORD 2----------------------\n summary | stddev              \n cont1   | 0.1876401764138863  \n cont2   | 0.20720173860981356 \n cont3   | 0.20210460819343745 \n cont4   | 0.21129221269283563 \n cont5   | 0.20902682854450405 \n cont6   | 0.20527256983553036 \n cont7   | 0.17845016396070926 \n cont8   | 0.19937045456133273 \n cont9   | 0.1816601713507561  \n cont10  | 0.18587672593201837 \n cont11  | 0.20973651144747804 \n cont12  | 0.20942662107602888 \n cont13  | 0.2127772423224096  \n cont14  | 0.22248753955922573 \n-RECORD 3----------------------\n summary | min                 \n cont1   | 1.6E-5              \n cont2   | 0.001149            \n cont3   | 0.002634            \n cont4   | 0.176921            \n cont5   | 0.281143            \n cont6   | 0.012683            \n cont7   | 0.069503            \n cont8   | 0.23688             \n cont9   | 8.0E-5              \n cont10  | 0.0                 \n cont11  | 0.035321            \n cont12  | 0.036232            \n cont13  | 2.28E-4             \n cont14  | 0.179722            \n-RECORD 4----------------------\n summary | max                 \n cont1   | 0.984975            \n cont2   | 0.862654            \n cont3   | 0.944251            \n cont4   | 0.954297            \n cont5   | 0.983674            \n cont6   | 0.997162            \n cont7   | 1.0                 \n cont8   | 0.9802              \n cont9   | 0.9954              \n cont10  | 0.99498             \n cont11  | 0.998742            \n cont12  | 0.998484            \n cont13  | 0.988494            \n cont14  | 0.844848            \n\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["The last column we need to explore is our outcome variable, `loss`.  The variable looks to be a continuous-valued positive number, ranging from 0.67 to 121,012.25.  The plot below displays the heavy-tailed distribution of data.  The variable has a very wide range of values with some high outliers given that the mean is greater than the median.  We may want to consider filtering out the extremeley high valued data points and treating them as outliers or or using a log-transform to make them closer to the same scale. We lean towards the latter since we do not have greater insight into the dataset to justify grooming the data, however our ultimate algorithm choice will drive our decision."],"metadata":{}},{"cell_type":"code","source":["# Plot the outcome variable, loss\nfig, ax = plt.subplots(figsize=(12,4))\nsns.distplot(s_df.select('loss').toPandas(), bins = 100, kde=False, rug=False)\nax.set_title('Distribution of the Outcome Variable')\nax.set_ylabel('Number of Claims')\nax.set_xlabel('Loss')\nmean = s_df.select(F.mean(\"loss\")).collect()\nmedian = s_df.approxQuantile(\"loss\", [0.5], 0.05)\nax.axvline(mean[0][\"avg(loss)\"], color='red', linestyle='--')\nax.axvline(median[0], color='green', linestyle='--')\nplt.legend({'Mean':mean,'Median':median})\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#### Preliminarily identify features for use\n\nThe column labels and values for the dataset features unfortunately do not provide clues on the meaning of the data contained therein:  they appear deliberately obfuscated.  While this is not a hindrance to designing a predictive machine learning model, it will prevent us from making domain-based decisions on which features to include or how to design interaction terms.  We will need to rely on mathematically-based dimensionality reduction techniques for any model implementation selected. \n\nWe still need to understand the presence or absence of multicollinearity amongst our continuous features though.  Highly correlated features can increase standard error in regression solutions, so we need to know if we should consider future corrective action.  The heatmap below visualizes the correlation matrix for continuous features.  We observe a few instances here that are rather high in magnitude, most notably between `cont1` and `cont9`, with a correlation coefficient of 0.92.  We note this occurrence as we will likely want to design a solution that mitigates this multicollinearity."],"metadata":{}},{"cell_type":"code","source":["# Convert each row's continuous valued features to a vector \nvector_col = \"corr_features\"\nassembler = VectorAssembler(inputCols=s_cont_df.columns, outputCol=vector_col)\ndf_vector = assembler.transform(s_cont_df).select(vector_col)\n\n# Calculate the correlation matrix\ncorr_df = Correlation.corr(df_vector, vector_col).toPandas()\ndense_matrix = corr_df.iloc[0]['pearson(corr_features)']\nrows = dense_matrix.toArray().tolist()\npdf = pd.DataFrame(rows, columns=s_cont_df.columns, index=s_cont_df.columns)\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(pdf, dtype=np.bool))\nfig, ax = plt.subplots(figsize=(8, 6))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(pdf, mask=mask, cmap=cmap, vmax=.9, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nax.set_title('Correlation Matrix of Continuous Valued Features')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["print(\"Correlation Matrix Values\")\npdf"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cont1</th>\n      <th>cont2</th>\n      <th>cont3</th>\n      <th>cont4</th>\n      <th>cont5</th>\n      <th>cont6</th>\n      <th>cont7</th>\n      <th>cont8</th>\n      <th>cont9</th>\n      <th>cont10</th>\n      <th>cont11</th>\n      <th>cont12</th>\n      <th>cont13</th>\n      <th>cont14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>cont1</th>\n      <td>1.000000</td>\n      <td>-0.085180</td>\n      <td>-0.445431</td>\n      <td>0.367549</td>\n      <td>-0.025230</td>\n      <td>0.758315</td>\n      <td>0.367384</td>\n      <td>0.361163</td>\n      <td>0.929912</td>\n      <td>0.808551</td>\n      <td>0.596090</td>\n      <td>0.614225</td>\n      <td>0.534850</td>\n      <td>0.056688</td>\n    </tr>\n    <tr>\n      <th>cont2</th>\n      <td>-0.085180</td>\n      <td>1.000000</td>\n      <td>0.455861</td>\n      <td>0.038693</td>\n      <td>0.191427</td>\n      <td>0.015864</td>\n      <td>0.048187</td>\n      <td>0.137468</td>\n      <td>-0.032729</td>\n      <td>0.063526</td>\n      <td>0.116824</td>\n      <td>0.106250</td>\n      <td>0.023335</td>\n      <td>-0.045584</td>\n    </tr>\n    <tr>\n      <th>cont3</th>\n      <td>-0.445431</td>\n      <td>0.455861</td>\n      <td>1.000000</td>\n      <td>-0.341633</td>\n      <td>0.089417</td>\n      <td>-0.349278</td>\n      <td>0.097516</td>\n      <td>-0.185432</td>\n      <td>-0.417054</td>\n      <td>-0.325562</td>\n      <td>0.025271</td>\n      <td>0.006111</td>\n      <td>-0.418203</td>\n      <td>-0.039592</td>\n    </tr>\n    <tr>\n      <th>cont4</th>\n      <td>0.367549</td>\n      <td>0.038693</td>\n      <td>-0.341633</td>\n      <td>1.000000</td>\n      <td>0.163748</td>\n      <td>0.220932</td>\n      <td>-0.115064</td>\n      <td>0.528740</td>\n      <td>0.328961</td>\n      <td>0.283294</td>\n      <td>0.120927</td>\n      <td>0.130453</td>\n      <td>0.179342</td>\n      <td>0.017445</td>\n    </tr>\n    <tr>\n      <th>cont5</th>\n      <td>-0.025230</td>\n      <td>0.191427</td>\n      <td>0.089417</td>\n      <td>0.163748</td>\n      <td>1.000000</td>\n      <td>-0.149810</td>\n      <td>-0.249344</td>\n      <td>0.009015</td>\n      <td>-0.088202</td>\n      <td>-0.064967</td>\n      <td>-0.151548</td>\n      <td>-0.148217</td>\n      <td>-0.082915</td>\n      <td>-0.021638</td>\n    </tr>\n    <tr>\n      <th>cont6</th>\n      <td>0.758315</td>\n      <td>0.015864</td>\n      <td>-0.349278</td>\n      <td>0.220932</td>\n      <td>-0.149810</td>\n      <td>1.000000</td>\n      <td>0.658918</td>\n      <td>0.437437</td>\n      <td>0.797544</td>\n      <td>0.883351</td>\n      <td>0.773745</td>\n      <td>0.785144</td>\n      <td>0.815091</td>\n      <td>0.042178</td>\n    </tr>\n    <tr>\n      <th>cont7</th>\n      <td>0.367384</td>\n      <td>0.048187</td>\n      <td>0.097516</td>\n      <td>-0.115064</td>\n      <td>-0.249344</td>\n      <td>0.658918</td>\n      <td>1.000000</td>\n      <td>0.142042</td>\n      <td>0.384343</td>\n      <td>0.492621</td>\n      <td>0.747108</td>\n      <td>0.742712</td>\n      <td>0.288395</td>\n      <td>0.022286</td>\n    </tr>\n    <tr>\n      <th>cont8</th>\n      <td>0.361163</td>\n      <td>0.137468</td>\n      <td>-0.185432</td>\n      <td>0.528740</td>\n      <td>0.009015</td>\n      <td>0.437437</td>\n      <td>0.142042</td>\n      <td>1.000000</td>\n      <td>0.452658</td>\n      <td>0.336588</td>\n      <td>0.302381</td>\n      <td>0.315904</td>\n      <td>0.476402</td>\n      <td>0.043539</td>\n    </tr>\n    <tr>\n      <th>cont9</th>\n      <td>0.929912</td>\n      <td>-0.032729</td>\n      <td>-0.417054</td>\n      <td>0.328961</td>\n      <td>-0.088202</td>\n      <td>0.797544</td>\n      <td>0.384343</td>\n      <td>0.452658</td>\n      <td>1.000000</td>\n      <td>0.785697</td>\n      <td>0.608000</td>\n      <td>0.626656</td>\n      <td>0.642028</td>\n      <td>0.074154</td>\n    </tr>\n    <tr>\n      <th>cont10</th>\n      <td>0.808551</td>\n      <td>0.063526</td>\n      <td>-0.325562</td>\n      <td>0.283294</td>\n      <td>-0.064967</td>\n      <td>0.883351</td>\n      <td>0.492621</td>\n      <td>0.336588</td>\n      <td>0.785697</td>\n      <td>1.000000</td>\n      <td>0.702896</td>\n      <td>0.713812</td>\n      <td>0.707876</td>\n      <td>0.041808</td>\n    </tr>\n    <tr>\n      <th>cont11</th>\n      <td>0.596090</td>\n      <td>0.116824</td>\n      <td>0.025271</td>\n      <td>0.120927</td>\n      <td>-0.151548</td>\n      <td>0.773745</td>\n      <td>0.747108</td>\n      <td>0.302381</td>\n      <td>0.608000</td>\n      <td>0.702896</td>\n      <td>1.000000</td>\n      <td>0.994384</td>\n      <td>0.466247</td>\n      <td>0.047293</td>\n    </tr>\n    <tr>\n      <th>cont12</th>\n      <td>0.614225</td>\n      <td>0.106250</td>\n      <td>0.006111</td>\n      <td>0.130453</td>\n      <td>-0.148217</td>\n      <td>0.785144</td>\n      <td>0.742712</td>\n      <td>0.315904</td>\n      <td>0.626656</td>\n      <td>0.713812</td>\n      <td>0.994384</td>\n      <td>1.000000</td>\n      <td>0.478677</td>\n      <td>0.050267</td>\n    </tr>\n    <tr>\n      <th>cont13</th>\n      <td>0.534850</td>\n      <td>0.023335</td>\n      <td>-0.418203</td>\n      <td>0.179342</td>\n      <td>-0.082915</td>\n      <td>0.815091</td>\n      <td>0.288395</td>\n      <td>0.476402</td>\n      <td>0.642028</td>\n      <td>0.707876</td>\n      <td>0.466247</td>\n      <td>0.478677</td>\n      <td>1.000000</td>\n      <td>0.047543</td>\n    </tr>\n    <tr>\n      <th>cont14</th>\n      <td>0.056688</td>\n      <td>-0.045584</td>\n      <td>-0.039592</td>\n      <td>0.017445</td>\n      <td>-0.021638</td>\n      <td>0.042178</td>\n      <td>0.022286</td>\n      <td>0.043539</td>\n      <td>0.074154</td>\n      <td>0.041808</td>\n      <td>0.047293</td>\n      <td>0.050267</td>\n      <td>0.047543</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["#### Store the data, inclusive of any transformations/corrections, for further analysis and scalable processing\n\nWe choose to store our initial CSV dataset as a parquet file. This should provide us with several advantages moving forward:\n  * It will eliminate any future overhead of reading in CSV files. \n  * It will greatly reduce the size of data we store on disk and, consequently, reduce network shuffling when we perform distributed processing\n  * It is well suited to what we envision as a batch operation for the business:  We do not anticipate updates to this model to be done with streaming data.  Thus, parquet is a smarter choice than avro.  \n\nWe confirm the savings in storage resources immediately:  whereas our CSV initially was initially 0.065 GB, the new parquet file is 0.01 GB.  The new parquet files require only 15% of the storage that the original CSV file did."],"metadata":{}},{"cell_type":"code","source":["# Write the CSV file to disk in Parquet format\ns_df.write.format(\"parquet\").save(unpacked_data_dir+\"train.parquet\")\n\n# Print training filename and size\nsum = 0\nfor item in dbutils.fs.ls(unpacked_data_dir + \"train.parquet\"):\n  sum += item.size\nprint(\"* Summed Size of Parquet Files:\", sum / 2**30, \"GB\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">* Summed Size of Parquet Files: 0.010068786330521107 GB\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["# Feature engineering"],"metadata":{}},{"cell_type":"markdown","source":["As indicated in our analysis above, the column names are coded and their meaning obscured. This has both advantages and disadvantages: we are unlikely to be biased in dropping variables that we believe have nothing to do with claim value but may have influence, but on the other hand our model may be crowded with multiple variables that do little to explain variance in the outcome variable `loss`. In practical terms, we will therefore not be dropping any columns - but we will be performing feature engineering to improve the performance of any model.\n\nFor the categorical variables described above, depending on the algorithms we explore we have several options. \n\nFirst is one-hot encoding - where each categorical value is given a new column and a binary representation for presence or absence. This increases the dimensionality of the dataset exponentially, however, and works best with dimensionality reduction or regularization. There are a variety of methods to accomplish this, which we will explore in the next section. \n\nSecond is Breiman’s method (for use in decision tree algorithms) - with the number of possible categorical values reaching 326 for some of our variables, Breiman’s method will help us find the best split for the categorical attribute without evaluating all possible subsets. In essence, Breiman’s method takes all possible categorical values and sorts them by average `y` value. So if there are 10 possible categorical values for one variable, Breiman’s method makes the splits based on sorting the average `loss` per category. This method of partitioning based on the mean behavior in the `y` value drastically reduces the computation needed. We will explore this in the next section as well.\n\nFinally, we could eliminate the correlated features that were identified above, but we will address this during modelling with dimensionality reduction techniques or regression regularization."],"metadata":{}},{"cell_type":"markdown","source":["#### sklearn Feature Engineering"],"metadata":{}},{"cell_type":"code","source":["# Read the training data into a pandas dataframe\ndf_train = pd.read_csv(\"/dbfs/user/john.boudreaux@ischool.berkeley.edu/FINAL_PROJECT/train.csv\")\n\n# Create a dataframe, filtered to remove the unique identifier and the loss columns\ndf_train_x = df_train.loc[:, (df_train.columns != 'loss') & (df_train.columns != 'id')]\n\n# Because the only data we have labels for is the training dataset, split it into test/train datasets\nx_train, x_test, y_train, y_test = train_test_split(df_train_x, df_train.loss, test_size=0.2, random_state=random_seed)\n\n# Log transform the outcome variable.\ny_train_log = np.log(y_train)\ny_test_log = np.log(y_test)\n\n\n################################ LINEAR / LASSO TRANSFORMATIONS ################################ \n\n# Perform one-hot encoding on categorical features for Lasso & Linear Regression.\ncategorical_cols = df_train.columns[['cat' in val for val in df_train.columns.values]]\ncategorical_transformer = skPipeline(steps=[('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))])\n\n# Any features that do not have a specific transformer, passthrough\npreprocessor = ColumnTransformer(transformers=\n                                 [('categorical', categorical_transformer, categorical_cols)],\n                                 remainder=\"passthrough\")\n\n########################## RANDOM FOREST REGRESSOR TRANSFORMATIONS #############################\n\n# Class to transform categorical data using Brieman's Method \nclass MyEncoder(BaseEstimator, TransformerMixin):\n  \n  def __init__(self, columns):\n    self.columns = columns\n    self.encoders = dict.fromkeys(columns, None)\n    \n  def transform(self, X, y=None):\n    # Using the dictionary of Brieman values, convert categorical entries to numbers\n    res = pd.DataFrame()\n    for column in X:\n      if column != 'y':      \n        res[column] = X[column].map(self.encoders[column])\n    return res\n  \n  def fit(self, X, y = None):\n    # Build the dictionary of Brieman values for each column's unique categorical values\n    y_median = np.median(X['y'])   # For any key we may see that we have not seen, set its value to the median of y\n    for column in X:\n      xag = X.groupby([column])['y']\n      dd = defaultdict(lambda: y_median)\n      for name, group in xag:\n        dd[name] = np.median(group)\n      self.encoders[column] = dd.copy()\n      dd = None\n    return self\n\n# To transform using Brieman's method, we need the y variable to be passed as part of the data to the \n# pre-processing transformer.  We will remove it once Brieman's method has been applied.\nx_all = x_train.copy()\nx_all['y']  = y_train\n    \nmergedlist = ['y']\nmergedlist.extend(categorical_cols)\n\nrandforest_preprocessor = skPipeline(steps = [('brieman', MyEncoder(mergedlist))])\n\n# Fit and transform the column transformer for Linear/Lasso (e.g., column feature engineering) based on the training data.\ndf_train_x_preprocessed = preprocessor.fit_transform(x_train)\n\n# Fit and transform the transformer for Random Forest \ndf_train_x_rfpreprocessed = randforest_preprocessor.fit_transform(x_all)\n\n# Print out raw data dataframe shape and transformed data dataframe shapes\nprint(\"                        Raw Dataframe Size:\", x_train.shape)\nprint(\" OLS/Ridge/Lasso, Post-Feature Engineering:\", df_train_x_preprocessed.shape)\nprint(\"   Random Forest, Post-Feature Engineering:\", df_train_x_rfpreprocessed.shape)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">                        Raw Dataframe Size: (150654, 130)\n OLS/Ridge/Lasso, Post-Feature Engineering: (150654, 1133)\n   Random Forest, Post-Feature Engineering: (150654, 130)\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["### PySpark Feature Engineering"],"metadata":{}},{"cell_type":"code","source":["# convenience function for one-hot encoding\ndef stageCreator(inputDf):\n  stages = []\n  cat_cols = [w for w in s_df.columns if 'cat' in w]\n  cont_cols = [w for w in s_df.columns if 'cont' in w]\n\n  for cc in cat_cols:\n    stringIndexer = StringIndexer(inputCol = cc, outputCol = cc + '_index')\n    encoder = OneHotEncoderEstimator(inputCols = [stringIndexer.getOutputCol()], outputCols = [cc + \"_vec\"])\n    stages += [stringIndexer, encoder]\n\n  assembler_inputs = [c + \"_vec\" for c in cat_cols] + cont_cols\n\n  assembler = VectorAssembler(inputCols = assembler_inputs, outputCol = \"features\")\n  stages += [assembler]\n  \n  return stages"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["# for ease with working with pyspark API\ns_df = s_df.withColumn('label', s_df.loss)\n\ns_train, s_test = s_df.randomSplit([0.8, 0.2], seed=random_seed)\n\nlinear_pipe_encoding_stages = stageCreator(s_train)\n\n# while we could run these pipeline steps for each linear/ridge/lasso model, \n# we can also pre-run them and have them set as an input dataFrame for each to save time\n\nlin_pipeline = Pipeline(stages = linear_pipe_encoding_stages).fit(s_train)\ns_df_train_onehot = lin_pipeline.transform(s_train)\ns_df_test_onehot = lin_pipeline.transform(s_test)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["While we did attempt to write a true pipeline step in the PySpark API to include, there were some particularly tricky aspects that prevented our group from getting them done in time. The first implementation on a private cluster (1 master node, 1 worker) took 19 hours to encode due to joins occuring per column to keep consistent with the API of having a single transformer for a single column.\n\nIn order to ensure that we had some sort of pipeline to continue a real apples-to-apples comparison between PySpark and Sklearn, we use our Breiman encoded data from our Pandas dataframe and parallelize it into a Spark dataframe."],"metadata":{}},{"cell_type":"code","source":["# create a pyspark df of our Breiman encoding\n\n# need the loss variable\ndf_train_temp = df_train_x_rfpreprocessed\ndf_train_temp['loss'] = y_train\n\ns_df_train_rf = spark.createDataFrame(df_train_temp)\ns_df_train_rf = s_df_train_rf.withColumn('label', s_df_train_rf.loss)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/session.py:577: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n  arrow_schema = pa.Schema.from_pandas(pdf, preserve_index=False)\n</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["### Algorithm Exploration\nGiven that we will retain the entire feature set due to its obfuscated meaning and the fact that we will be predicting MAE, a continuous value, we pair a form of dimensionality reduction with four regression-based approaches to initially explore with our dataset.  \n\n**Linear Models:  Ordinary Least Squares (OLS) , Ridge, and Lasso**\n\nWe first begin by using an **OLS** regression approach as it provides a simple, but often very effective means by which to predict continuous valued outcomes and guide more nuanced regression approaches (ISLR Chapter 3).  In addition, in its basic form and many of its advanced forms, it is a scalable approach to large data sets.  In fact, Spark's scalable machine learning library, MLLIB, provides native support for the algorithm and many of its permutations, making it an optimized choice for large data set learning and prediction.  \n\n\nWe pair the initial OLS approach with principal component analysis (PCA) to address the large feature space resulting from our inability to apply domain knowledge to downselect features and our use of one-hot encoding for the 116 categorical variables.  Recall from our feature engineering that the application of one-hot encoding expanded our number of feature columns from 130 to 1133 and left us with a highly sparse dataset.  This *curse of dimensionality* most often introduces noise into a model's predictive abilities and PCA is a statistical aproach that will help reduce our feature set to those that provide the strongest signal for our outcome (ISLR Chapter 6).  To preliminarily understand the impact of reduction on the algorithm, we will perform a gridsearch on the optimal number of principal components to include in the regression.\n\nTo build on OLS, we will then examine if we see improved predictive performance with a **Ridge Regression** approach.  Ridge regression will introduce L2 regularization to our base OLS approach, providing a means by which we can reduce potential negative effects of the mild multicollinearity we identified in the EDA section and by which we can trade a small increase in bias for a larger decrease in variance due to the large number of features in our one-hot encoded dataset (ISLR Chapter 6).  We will similarly apply **Lasso Regression** to the dataset to see if L1 regularization is more performant.  For this algorithm, we will remove PCA preprocessing and allow the L1 penalty to self-identify the features that matter most.  Lasso regression will essentially remove features it deems uninformative to our outcome variable by zeroing the associated feature's coefficient (ISLR Chapter 6). For both the Ridge and Lasso Regression approaches, we will use gridsearch to examine several alpha values and gain a cursory understanding of the performance we might expect to see should we fine-tune the approaches.\n\n** Tree Models:  Random Forest Regression **\n\nFinally, we plan to vet how **Random Forest Regression** performs with our dataset.  Random Forest, an ensembling approach to decision trees, is highly desirable given our dataset.  Besides being an iterative algorithm and innately scalable, it allows us to almost naturally explore interaction terms in our data thru the sequential splits in each tree:  given the obfuscated nature of our data, this is advantageous since we were not able to apply domain knowledge to the dataset to derive or explore such possible interactions.  In addition, because a random forest is made up of many trees, grown independently, with only a subset of columns, the algorithm also attempts to mitigate against multicollinearity, a possible problem we noted.  Finally, from an implementation perspective, the algorithm only requires us to tune a handful of hyperparameters, making it an approach we should be able to fine tune with confidence.  \n\nTo explore the algorithm, we will apply Breiman's method to transform the categorical variables into numeric values.  This should provide an advantage over one-hot encoding which greatly exploded the number of columns in our dataset.  To explore the variation we may see with tuning, we will gridsearch on max_depth.\n\n** Baseline Expectations ** \n\nTo establish our expectations for these algorithm choices, we take the median value of the outcome variable, MAE, in the dataset.  With a value of **2115.57**, this is our crudest baseline estimate.  We also examine the the Kaggle leaderboard to understand the baseline results of a fine-tuned model:  the result is **1109.70772** (https://www.kaggle.com/c/allstate-claims-severity/leaderboard).  These two values establish a watermark by which we can understand the results of our algorithm exploration.\n\n\n** Results ** \n\nUsing a cross-validated gridsearch on a single hyperparamter for each approach, we record our best results as follows:\n  - OLS Regression:  1303.017143\n  - Ridge Regression:  1303.080524\n  - Lasso Regression:  1296.568151\n  - Random Forest Regression:  1212.194926\n  \nAll of the algorithms far outperform our crude estimate of a mean and Random Forest Regression, by far, outperforms the other algorithms.  The plots below depict the performance of each approach.  Based on this data, we will choose to fine tune and implement a Random Forest Regression solution since it gave the best MAE on a limited hyperparameter sweep."],"metadata":{}},{"cell_type":"markdown","source":["#### Spark Algorithm Exploration"],"metadata":{}},{"cell_type":"code","source":["# define helper functions for extracting models from pyspark cross-validated model objects\n\ndef extract_crossval_info_ridgelasso(cvModelObj):\n  # first extract the mean of the MAE's\n  # extracts the regularization parameter\n  maes = []\n  folds = []\n  params = []\n  \n  i = 1\n  for fold in cvModelObj.subModels:\n    for model in fold:\n      folds.append(i)\n      maes.append(model.stages[-1].summary.meanAbsoluteError)\n      params.append(model.stages[-1]._java_obj.getRegParam())\n    i += 1\n  \n  df = pd.DataFrame({'fold' : folds, 'mae' : maes, 'param' : params})\n  \n  return(df)\n\ndef extract_crossval_info_linear(cvModelObj):\n  # first extract the mean of the MAE's\n  # extracts the PCA component number\n  maes = []\n  folds = []\n  params = []\n  \n  i = 1\n  for fold in cvModelObj.subModels:\n    for model in fold:\n      folds.append(i)\n      maes.append(model.stages[-1].summary.meanAbsoluteError)\n      params.append(model.stages[-2]._java_obj.getK())\n    i += 1\n  \n  df = pd.DataFrame({'fold' : folds, 'mae' : maes, 'param' : params})\n  \n  return(df)\n\ndef create_lineplot(cvModelObj, title, param, type_model):\n  # creates a line plot for the average MAE for each parameter value\n  \n  # pull out appropriate parameter from functions defined above\n  if type_model == 'linear':\n    info = extract_crossval_info_linear(cvModelObj)\n  elif type_model == 'ridgelasso':\n    info = extract_crossval_info_ridgelasso(cvModelObj)\n    \n  grouped = info.groupby('param').mean()\n  \n  #plotting\n  ret_plot = sns.lineplot(x = grouped.index, y = grouped.mae)\n  ret_plot.set_title(title)\n  ret_plot.set_ylabel('Mean Absolute Error')\n  ret_plot.set_xlabel(param)\n  \n  # hard code axis values for easy comparison\n  ret_plot.set_ylim(1200, 1700)\n  \n  return ret_plot"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["# linear regression \n\nlinreg_stages = []\n\n# if we wanted to include the full pipeline, we would change datasource to s_df\n# and uncomment next line\n# linreg_stages = stageCreator(s_train_small)\npca_stage = ps_PCA(inputCol = \"features\", outputCol = \"pcaFeatures\")\nlinreg_stages.append(pca_stage)\nlinreg_stage = ps_linreg(featuresCol = pca_stage.getOutputCol(), labelCol = 'label', maxIter = 10)\nlinreg_stages.append(linreg_stage)\n\npipelineLinReg = Pipeline(stages = linreg_stages)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(pca_stage.k, np.random.randint(500, size = 10).tolist()) \\\n    .build()\n\nevaluator = RegressionEvaluator(predictionCol = 'prediction', labelCol = 'label', metricName = \"mae\")\n\ncrossval = CrossValidator(estimator=pipelineLinReg,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3,\n                          collectSubModels=True)\n\ncvModel_linear = crossval.fit(s_df_train_onehot)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["# linear regression plotting\ndisplay(create_lineplot(cvModel_linear, \"Linear Regression with PCA\", \"Number of PCA features\", 'linear'))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# ridge regression\nridge_stages = []\n\n# if we wanted to include the full pipeline, we would change datasource to s_df\n# and uncomment next line\n\n# ridge_stages = stageCreator(s_train)\nridge_stage = ps_linreg(featuresCol = \"features\", labelCol = 'label', elasticNetParam=0.0)\nridge_stages.append(ridge_stage)\n\npipelineRidge = Pipeline(stages = ridge_stages)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(ridge_stage.regParam,  np.random.uniform(low=0.0001, high=.1, size=20).tolist()) \\\n    .build()\n\nevaluator = RegressionEvaluator(predictionCol = 'prediction', labelCol = 'label', metricName = \"mae\")\n\ncrossval = CrossValidator(estimator=pipelineRidge,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3,\n                          collectSubModels=True)\n\ncvModel_ridge = crossval.fit(s_df_train_onehot)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["# ridge regression plotting\ndisplay(create_lineplot(cvModel_ridge, \"Ridge Regression\", \"Alpha\", \"ridgelasso\"))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# lasso regression\n\nlasso_stages = []\n# lasso_stages = stageCreator(s_train)\nlasso_stage = ps_linreg(featuresCol = \"features\", labelCol = 'label', elasticNetParam=1.0)\nlasso_stages.append(lasso_stage)\n\npipelineLasso = Pipeline(stages = lasso_stages)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lasso_stage.regParam,  np.random.uniform(low=0.0001, high=.1, size=10).tolist()) \\\n    .build()\n\nevaluator = RegressionEvaluator(predictionCol = 'prediction', labelCol = 'label', metricName = \"mae\")\n\ncrossval = CrossValidator(estimator=pipelineLasso,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3,\n                          collectSubModels = True)\n\ncvModel_lasso = crossval.fit(s_df_train_onehot)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["# lasso regression plotting\ndisplay(create_lineplot(cvModel_lasso, \"Lasso Regression\", \"Alpha\", \"ridgelasso\"))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# random forest\n# Identify the categorical column names\ncat_cols = [w for w in s_df_train_rf.columns if 'cat' in w]\n\n# Identify the continuous valued column names\ncont_cols = [w for w in s_df_train_rf.columns if 'cont' in w]\n\n# first need features in a vector\nrelevant_cols = cat_cols + cont_cols\nvectorizer = VectorAssembler(inputCols = relevant_cols, outputCol = \"features\")\n\n# now define rf\nrf_model = ps_rf(featuresCol = 'features', labelCol = 'label')\n\npipelinerf = Pipeline(stages = [vectorizer, rf_model])\n\n# parameter grid\nparamGrid = ParamGridBuilder() \\\n    .addGrid(rf_model.maxDepth, np.random.randint(low=2, high=20, size = 10).tolist())\\\n    .build()\n\nevaluator = RegressionEvaluator(predictionCol = 'prediction', labelCol = 'label', metricName = \"mae\")\n\ncrossval = CrossValidator(estimator=pipelinerf,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3,\n                          collectSubModels = True)\n\ncvModel_rf = crossval.fit(s_df_train_rf)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["print(\"Best Max Depth: \" + str(cvModel_rf.bestModel.stages[-1].getOrDefault('maxDepth')))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Best Max Depth: 18\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["rf_maes = cvModel_rf.avgMetrics\nrf_params = [list(cvModel_rf.getEstimatorParamMaps()[i].values())[0] for i in range(len(cvModel_rf.getEstimatorParamMaps()))]\n\n#plotting\nret_plot = sns.lineplot(x = rf_params, y = rf_maes)\nret_plot.set_title(\"Random Forest Regression\")\nret_plot.set_ylabel('Mean Absolute Error')\nret_plot.set_xlabel(\"Max Depth\")\n\n# hard code axis values for easy comparison\nret_plot.set_ylim(1200, 1700)\n\ndisplay(ret_plot)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["#### sklearn Algorithm Exploration"],"metadata":{}},{"cell_type":"code","source":["# Calculate the median of the outcome variable as a baseline.\nprint(\"==>  Median MAE for training dataset:\", df_train.loss.median())\n\n# Define a pipeline for Linear Regression, using PCA for dimensionality reduction \npipeline_lr = skPipeline(steps=[('preprocess', preprocessor), \n                                ('pca', PCA(n_components=2)),\n                                ('classifier', LinearRegression())])\n\npipeline_ridge = skPipeline(steps=[('preprocess', preprocessor),\n                                   ('pca', PCA(n_components=350)),\n                                   ('classifier', linear_model.Ridge())])\n\n# Define a pipeline for Lasso Regression using L1 regularization for dimensionality reduction\npipeline_lasso = skPipeline(steps=[('preprocess', preprocessor),\n                                   ('classifier', Lasso())])\n\n# Define a pipeline for Random Forest using Brieman's method for transforming categorical features.\npipeline_rf = skPipeline(steps=[('preprocess', randforest_preprocessor), \n                                ('classifier', RandomForestRegressor(max_depth=2, random_state=random_seed, n_estimators=10))])\n\n# Build a list of our sklearn pipelines\npipelines = [pipeline_rf, pipeline_lr, pipeline_lasso, pipeline_ridge]\npipeline_names = ['Random Forest Regressor', 'Linear Regression', 'Lasso Regression', 'Ridge Regression']\n\n# Define hyperparameter ranges for gridsearch\nhyperparameter_grid = {'Linear Regression': {'pca__n_components': np.random.randint(500, size = 10)},\n                       'Lasso Regression': {'classifier__alpha': np.random.uniform(low=0.0001, high=.1, size=10)},\n                       'Ridge Regression': {'classifier__alpha': np.random.uniform(low=0.0001, high=.1, size=10)},\n                       'Random Forest Regressor': {'classifier__max_depth': np.random.randint(low=2, high=20, size = 10)}\n                      }\n\nresults_params = []\nresults_scores = []\n\n# For each pipeline, find the best hyperparameters using gridsearch and the mean absolute error metric\nfor i, p in enumerate(pipelines):\n    grid = GridSearchCV(p, cv=3, param_grid=hyperparameter_grid[pipeline_names[i]], scoring='neg_mean_absolute_error')\n    if pipeline_names[i] == 'Random Forest Regressor':\n      grid.fit(x_all, y_train)\n    else:\n      grid.fit(x_train, y_train)\n    print(\"==>  %s Best: %f using %s\" % (pipeline_names[i], grid.best_score_, grid.best_params_))\n    results_params.append(grid.cv_results_['params'])\n    results_scores.append(grid.cv_results_['mean_test_score'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">==&gt;  Random Forest Regressor Best: -1212.194926 using {&#39;classifier__max_depth&#39;: 14}\n==&gt;  Linear Regression Best: -1303.017143 using {&#39;pca__n_components&#39;: 367}\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n/databricks/python/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n==&gt;  Lasso Regression Best: -1296.568151 using {&#39;classifier__alpha&#39;: 0.09335016803529422}\n==&gt;  Ridge Regression Best: -1303.080524 using {&#39;classifier__alpha&#39;: 0.094521646773581}\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":["# Plot the results of our algorithm exploration.\nfig, ax1 = plt.subplots(1, 4, figsize=(12, 4), sharey = True)\n\npca_n = [d['pca__n_components'] for d in results_params[1]]\nsns.lineplot(x=pca_n, y=abs(results_scores[1]), ax=ax1[0])\nax1[0].set_title('OLS Regression')\nax1[0].set_xlabel('PCA Components')\n\nalphas = [d['classifier__alpha'] for d in results_params[3]]\nsns.lineplot(x=alphas, y=abs(results_scores[3]), ax=ax1[1])\nax1[1].set_title('Ridge Regression')\nax1[1].set_xlabel('Alpha')\n\nalphas = [d['classifier__alpha'] for d in results_params[2]]\nsns.lineplot(x=alphas, y=abs(results_scores[2]), ax=ax1[2])\nax1[2].set_title('Lasso Regression')\nax1[2].set_xlabel('Alpha')\n\nmax_depths_n = [d['classifier__max_depth'] for d in results_params[0]]\nsns.lineplot(x=max_depths_n, y=abs(results_scores[0]), ax=ax1[3])\nax1[3].set_title('Random Forest Regression')\nax1[3].set_ylabel('Mean Absolute Error')\nax1[3].set_xlabel('Max Depth')\n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["# Algorithm Implementation"],"metadata":{}},{"cell_type":"markdown","source":["Since the random forest algorithm performed best in our basic hyperparameter tuning, we decided to do further exploration of how this algorithm works. The building block of a random forest is a decision tree which creates binary splits to subset the data into groups that minimize error (in our case, mean absolute error). In a most general procedure, we consider all possible split criteria for all possible input variables. So in our case for a sample dataset, we have `cat1`, `cat2`, and `cont_1` which represent two categorical variables and one continuous variable used to predict some outcome variable `y`."],"metadata":{}},{"cell_type":"code","source":["# mini data definition\nfrom pyspark.sql.types import *\nsample_schema = StructType([\n  StructField('cat1', IntegerType(), True),\n  StructField('cat2', IntegerType(), True),\n  StructField('cont_1', FloatType(), True),\n  StructField('y', FloatType(), True)\n])\n\nsample_df = spark.createDataFrame([\n            (1, 0, 0.97, 14.2),\n            (1, 1, 0.77, 13.4),\n            (0, 1, 0.81, 11.1),\n            (0, 0, 0.55, 9.8),\n            (1, 2, 0.25, 10.1)\n], schema = sample_schema)\n\nsample_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+------+----+\ncat1|cat2|cont_1|   y|\n+----+----+------+----+\n   1|   0|  0.97|14.2|\n   1|   1|  0.77|13.4|\n   0|   1|  0.81|11.1|\n   0|   0|  0.55| 9.8|\n   1|   2|  0.25|10.1|\n+----+----+------+----+\n\n</div>"]}}],"execution_count":49},{"cell_type":"markdown","source":["An important note that since we are using MAE as our error metric, we need to use medians rather than means as a baseline since this parameter is optimal for MAE estimation. Our median `y` value of the whole set is `11.1`, so this is our baseline prediction leading to a MAE of `((14.2 - 11.1) + (13.4 - 11.1) + (11.1 - 11.1) + (11.1 - 9.8) + (11.1 - 10.1)) / 5 = 1.54`.\n\nNow, we want to evaluate all possible ways to make a binary split on the data on a basis of the input parameters. There are two different types of inputs we might have- categorical or numerical. Numerical values have potentially lots of possible split points for making greather than/ less than comparisons which makes them more computationally difficult; most code implementations will have a cutoff on the number of possible splits in the data they they might consider. Categorical variables can be compared on a binary basis of if they are a particular category or not. We can also use Brieman's method to replace the categories with a relevant aggregation of the outcome variable grouped by the categories of the inputs; for instance, we can replace `cat1` for values of `cat1 = 1` with the median value of `y` when `cat = 1` ( in this case, `13.8`). Simlilarly, we can do the same for when `cat1=0`. Brieman's method gives us the advantage of being able to make numerical comparisons in addition to reducing the effective number of decisions that need to be made. In the case of binary categorical variables this won't make a difference, but for non-binary it certainly will. \n\nUsing Breiman's method, we can transform our data into the following:\n\n\n_Source for median being optimal for MAE, with derivation: https://medium.com/@gennadylaptev/median-and-mae-3e85f92df2d7)_\n\n_Source for Breiman's method: Async lecture section 12.8 for UC Berkeley Data Science w261_"],"metadata":{}},{"cell_type":"code","source":["# need to make user-defined function for aggregating medians\ndef median(values_list):\n    med = np.median(values_list)\n    return float(med)\nudf_median = F.udf(median, FloatType())\n\nbr_df = sample_df\n\n# note: this method does not appear to scale well\nfor i in range(1, 3):\n  cat_var_str = 'cat' + str(i)\n  tmp_cat_var_str = cat_var_str + \"_\"\n  br_var_str = 'br_cat' + str(i)\n\n#   print(\"Performing for \" + cat_var_str)\n  group_df = br_df.groupBy([cat_var_str])\n  df_grouped = group_df.agg(udf_median(F.collect_list('y')).alias('median'))\n  df_grouped = df_grouped.withColumnRenamed(cat_var_str, tmp_cat_var_str)\n\n  br_df = br_df.join(df_grouped, F.col(cat_var_str) == F.col(tmp_cat_var_str), how = 'left')\\\n              .withColumnRenamed('median', br_var_str).drop(tmp_cat_var_str)\n  \nbr_df = br_df.drop('cat1').drop('cat2')\nbr_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+----+---------+-------+\ncont_1|   y|  br_cat1|br_cat2|\n+------+----+---------+-------+\n  0.81|11.1|10.450001|  12.25|\n  0.77|13.4|     13.4|  12.25|\n  0.25|10.1|     13.4|   10.1|\n  0.97|14.2|     13.4|   12.0|\n  0.55| 9.8|10.450001|   12.0|\n+------+----+---------+-------+\n\n</div>"]}}],"execution_count":51},{"cell_type":"markdown","source":["So, Breiman's method now employed, we can move on to calculating the MAE of making every possible split in the data from our current node which contains all of the data. For our first categorical variable, all of the possible split points are considered in the calculations below:"],"metadata":{}},{"cell_type":"markdown","source":["> **Variable: `cat_1` (Breiman encoded)**\n> \n> split point 1: $$ brcat1 \\leq 13.4$$\n>\n> for the values that fit criteria (TRUE) we have 3 total data points with a median of `13.4`: \n> $$y_0 = 13.4,  y_2 = 10.1, y_3 = 14.2$$\n>\n> for the values that do not fit criteria (FALSE) we have 2 total data points with a median of `10.45`:\n> $$y_1 = 11.1, y_4 = 9.8$$\n>\n> Median Absolute Error (MAE) of split point: \n> $$ \n> MAE = \\frac{1}{n} ( \\sum |\\hat{y} - y|\\_{TRUE} +  \\sum|\\hat{y} -y|\\_{FALSE})\n> $$\n> $$ \n> MAE = \\frac{1}{5} ((|13.4 - 13.4| +  |10.1-13.4| + |14.2-13.4|) +(|11.1-10.45| + |9.8-10.45|))\n> $$\n> $$ \n> MAE = \\frac{1}{5} (0 +  3.3 + 0.8 + 0.65 + 0.65)\n> $$\n> $$ \n> MAE = \\frac{1}{5} (5.4) = 1.08\n> $$\n>\n> **Variable: `cat_2` (Breiman encoded)**\n> \n> split point 1: $$ brcat2 < 12$$\n>\n> for the values that fit criteria (TRUE) we have 1 total data points with a median of `10.1`: \n> $$y_1 = 10.1$$\n>\n> for the values that do not fit criteria (FALSE) we have 4 total data points with a median of `12.25`:\n> $$y_0 = 13.4, y_1 = 11.1, y_3 = 14.2, y_4 = 9.8$$\n>\n> Median Absolute Error (MAE) of split point: \n> $$ \n> MAE = \\frac{1}{5} ((|10.1 - 10.1|) +  (|13.4-12.25| + |11.1-12.25| +(|14.2-12.25| + |9.8-12.25|))\n> $$\n> $$ \n> MAE = 1.38\n> $$\n> \n> split point 2: $$ brcat2 < 12$$\n>\n> for the values that fit criteria (TRUE) we have 3 total data points with a median of `10.1`: \n> $$y_2 = 10.1, y_3 = 14.2, y_4 = 9.8$$\n>\n> for the values that do not fit criteria (FALSE) we have 2 total data points with a median of `12.25`:\n> $$y_0 = 13.4, y_1 = 11.1$$\n>\n> Median Absolute Error (MAE) of split point: \n> $$ \n> MAE = \\frac{1}{5} ((|10.1 - 10.1|) +  (|14.2-10.1| + |14.2-9.8| +(|13.4-12.25| + |11.1-12.25|))\n> $$\n> $$ \n> MAE = 2.16\n> $$\n>\n> **Variable: `cont_1`**\n> \n> split point 1: $$ cont1 < 0.55$$\n>\n> for the values that fit criteria (TRUE) we have 1 total data points with a median of `10.1`: \n> $$y_2 = 10.1$$\n>\n> for the values that do not fit criteria (FALSE) we have 4 total data points with a median of `12.25`:\n> $$y_0 = 13.4, y_1 = 11.1, y_3 = 14.2, y_4 = 9.8$$\n>\n> We notice this is an equivalent split to `br_cat2 < 12`. Therefore, \n> $$ \n> MAE = 13.4\n> $$\n> \n> split point 2: $$ cont1 < 0.77$$\n>\n> for the values that do not fit criteria (TRUE) we have 2 total data points with a median of `9.95`:\n> $$y_2 = 10.1, y_4 = 9.8$$\n>\n> for the values that fit criteria (FALSE) we have 3 total data points with a median of `13.4`: \n> $$y_0 = 13.4, y_1 = 11.1, y_3 = 14.2$$\n>\n> Median Absolute Error (MAE) of split point: \n> $$ \n> MAE = \\frac{1}{5} ((|13.4-13.4| + |13.4-11.1| + |13.4-14.2|) +(|10.1-9.95| + |9.8-9.95|))\n> $$\n> $$ \n> MAE = 0.882\n> $$\n>\n> split point 3: $$ cont1 < 0.81$$\n>\n> for the values that do not fit criteria (TRUE) we have 3 total data points with a median of `10.1`:\n> $$y_0 = 13.4, y_2 = 10.1, y_4 = 9.8$$\n>\n> for the values that fit criteria (FALSE) we have 2 total data points with a median of `12.65`: \n> $$y_1 = 11.1, y_3 = 14.2$$\n>\n> Median Absolute Error (MAE) of split point: \n> $$ \n> MAE = \\frac{1}{5} ((|11.1-12.65| + |14.2-12.65|) + (|13.4-10.1| + |10.1-10.1| + |9.8-10.1|))\n> $$\n> $$ \n> MAE = 1.34\n> $$\n>\n> split point 4: $$ cont1 < 0.97$$\n>\n> for the values that do not fit criteria (TRUE) we have 4 total data points with a median of `10.6`:\n> $$y_0 = 13.4, y_1 = 11.1, y_2 = 10.1, y_4 = 9.8$$\n>\n> for the values that fit criteria (FALSE) we have 1 total data points with a median of `14.2`: \n> $$y_3 = 14.2$$\n>\n> Median Absolute Error (MAE) of split point: \n> $$ \n> MAE = \\frac{1}{5} ((|14.2-14.2|) + (|13.4-10.6| + |11.1-10.6| + |10.1-10.6| + |9.8-10.6|))\n> $$\n> $$ \n> MAE = 0.92\n> $$"],"metadata":{}},{"cell_type":"markdown","source":["We see that splitting the data on `cont_1 < 0.77` provides child nodes that minimize MAE compared to all other split nodes, so this will be our choice of how to subset our data. We can continue this process for the two child nodes we created through this process theoretically until we have leaf nodes with just one example each. We note that this is a greedy algorithm at each stage. This would give us a decision tree that minimizes MAE for our training set (but may not generalize well).\n\nBringing this back to random forests, random forests are composed of many different decision trees (usually hundreds at a time). The main idea of doing this is to create lots of weak predictors which on an individual basis may not perform well, but when aggregated together perform very well. Each decision tree should be generated such that it is different than other trees, and there are a variety of ways to do that.\n\nWe construct each tree with a subset of the observations in the data, and might consider limited variables at each node for further splitting. Additionally, we typically cut off the tree to control for overfitting of individual trees; this can be done through pruning with an alpha parameter after the tree has been fully built out. We can also implement simpler methods that work towards the same purpose by putting a minimum number of data points that must be in a node to consider further splitting or specifying maximum depth. By controlling the proportion of data in each tree, features to be considered in split points, and number of split points to have overall, we can control the \"randomness\" of each tree in our forest. We can also tune for how many trees to be in the forest as an additional hyperparameter.\n\nRandom forests are great for distributed computing because they have a lot of levels of parallelization. First, each tree does not depend on any other tree; these can be trained independently. Additionally, the calculation of the next best split point can be parallelized, since the MAE calculation for each considered split point depends only on the data included in the node, not other split points being considered. We notice a big change in performance with using trees with MAE as an evaluation criteria rather than mean squared error. MSE minimizes variance in each child node, which can be calculated without a sort unlike medians."],"metadata":{}},{"cell_type":"markdown","source":["#### sklearn Algorithm Implementation"],"metadata":{}},{"cell_type":"code","source":["# Define a pipeline for Random Forest using Brieman's method for transforming categorical features.\npipeline_rf = skPipeline(steps=[('preprocess', randforest_preprocessor), \n                                ('classifier', RandomForestRegressor(random_state=random_seed))])\n\n# Define hyperparameter ranges for gridsearch\nrandomforest_grid = {'classifier__max_depth': [5, 9, 13, 15],\n                     'classifier__n_estimators': [100, 200, 300, 400],\n                     'classifier__max_features' : ['sqrt', 'log2', round((1/3) * df_train_x_rfpreprocessed.shape[1])]\n                    }\n\n# Execute gridsearch\ngrid = GridSearchCV(pipeline_rf, cv=10, param_grid=randomforest_grid, scoring='neg_mean_absolute_error')\ngrid.fit(x_all, y_train)\nprint(\"==>  Best: %f using %s\" % (grid.best_score_, grid.best_params_))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">==&gt;  Best: -1131.213809 using {&#39;classifier__max_depth&#39;: 13, &#39;classifier__max_features&#39;: 43, &#39;classifier__n_estimators&#39;: 400}\n</div>"]}}],"execution_count":56},{"cell_type":"code","source":["# Using the best fit model, generate predictions for the test dataset\npreds = grid.predict(x_test)\nscore = np.mean(np.abs(y_test - preds))\nprint(\"Final score on test dataset:\", score)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Final score on test dataset: 1293.7014615633975\n</div>"]}}],"execution_count":57},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(5,3))\ndiff = y_test - preds\nsns.distplot(diff, bins = 100, kde=False, rug=False)\nax.set_title('Distribution of Residuals')\nax.set_ylabel('Frequency')\nax.set_xlabel('Residual')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["### Pyspark Algorithm Implementation"],"metadata":{}},{"cell_type":"code","source":["# random forest\n# Identify the categorical column names\ncat_cols = [w for w in s_df_train_rf.columns if 'cat' in w]\n\n# Identify the continuous valued column names\ncont_cols = [w for w in s_df_train_rf.columns if 'cont' in w]\n\n# first need features in a vector\nrelevant_cols = cat_cols + cont_cols\nvectorizer = VectorAssembler(inputCols = relevant_cols, outputCol = \"features\")\n\n# now define rf\nrf_model = ps_rf(featuresCol = 'features', labelCol = 'label')\n\npipelinerf = Pipeline(stages = [vectorizer, rf_model])\n\n# parameter grid\nparamGrid = ParamGridBuilder() \\\n    .addGrid(rf_model.maxDepth, [5, 9, 13, 15])\\\n    .addGrid(rf_model.numTrees, [100, 200, 300, 400])\\\n    .addGrid(rf_model.featureSubsetStrategy, ['sqrt', 'log2', 'onethird'])\\\n    .build()\n\nevaluator = RegressionEvaluator(predictionCol = 'prediction', labelCol = 'label', metricName = \"mae\")\n\ncrossval = CrossValidator(estimator=pipelinerf,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=10,\n                          collectSubModels = False)\n\ncvModel_rf_final = crossval.fit(s_df_train_rf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n</div>"]}}],"execution_count":60},{"cell_type":"code","source":["# get training MAE\nevaluator.evaluate(cvModel_rf_final.transform(s_df_train_rf))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: 1019.3488995640754</div>"]}}],"execution_count":61},{"cell_type":"code","source":["# create spark version of test data\nx_all = x_test.copy()\nx_all['y']  = y_test\n    \nmergedlist = ['y']\nmergedlist.extend(categorical_cols)\n\nrandforest_preprocessor = skPipeline(steps = [('brieman', MyEncoder(mergedlist))])\n\n# Fit and transform the transformer for Random Forest \ndf_test_x_rfpreprocessed = randforest_preprocessor.fit_transform(x_all)\n\n# need the loss variable\ndf_test_temp = df_test_x_rfpreprocessed\ndf_test_temp['loss'] = y_test\n\ns_df_test_rf = spark.createDataFrame(df_test_temp)\ns_df_test_rf = s_df_test_rf.withColumn('label', s_df_test_rf.loss)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":62},{"cell_type":"code","source":["# get MAE on test set\nevaluator.evaluate(cvModel_rf_final.transform(s_df_test_rf))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: 1192.1552197823416</div>"]}}],"execution_count":63},{"cell_type":"code","source":["cvModel_rf_final.bestModel.stages[-1].extractParamMap()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: {Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;cacheNodeIds&#39;, doc=&#39;If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.&#39;): False,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;checkpointInterval&#39;, doc=&#39;set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext&#39;): 10,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;featureSubsetStrategy&#39;, doc=&#39;The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].&#39;): &#39;onethird&#39;,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;featuresCol&#39;, doc=&#39;features column name&#39;): &#39;features&#39;,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;impurity&#39;, doc=&#39;Criterion used for information gain calculation (case-insensitive). Supported options: variance&#39;): &#39;variance&#39;,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;labelCol&#39;, doc=&#39;label column name&#39;): &#39;label&#39;,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;maxBins&#39;, doc=&#39;Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.&#39;): 32,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 15,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;maxMemoryInMB&#39;, doc=&#39;Maximum memory in MB allocated to histogram aggregation.&#39;): 256,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;minInfoGain&#39;, doc=&#39;Minimum information gain for a split to be considered at a tree node.&#39;): 0.0,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;minInstancesPerNode&#39;, doc=&#39;Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.&#39;): 1,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (at least 1)&#39;): 400,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;predictionCol&#39;, doc=&#39;prediction column name&#39;): &#39;prediction&#39;,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;seed&#39;, doc=&#39;random seed&#39;): 2502083311556356884,\n Param(parent=&#39;RandomForestRegressor_fefab6be4a9e&#39;, name=&#39;subsamplingRate&#39;, doc=&#39;Fraction of the training data used for learning each decision tree, in range (0, 1].&#39;): 1.0}</div>"]}}],"execution_count":64},{"cell_type":"markdown","source":["From Pyspark, our best model parameters were `numTrees = 400`, `maxDepth = 15`, `featureSubsetStrategy = 'onethird'`. These number of trees and maximum features are the same as sklearn, but the maximum depth came back slightly different."],"metadata":{}},{"cell_type":"markdown","source":["# Conclusions\n\nOur first objective in this project was to explore machine learning algorithms to predict insurance claim losses. We found the random forest algorithm performed very near to the leaderboard results in Kaggle. It is clear that this algorithm design is ideal for this type of data: it allows us to explore multiple reactions between features in a random way (given our feature space is obscured), as well as being innately scalable. \n\nOur sklearn model had a MAE of 1131 on the training data, and 1293 on the test data. Our PySpark model had a MAE of 1019 on the training data, and 1192 on the test data. Although the kaggle leaderboard uses a different dataset for its own use, a MAE of 1293 would be around rank 2495 of 3045, and a MAE of 1192 would be 2140 of 3045 entries (a difference of about 350 places).\n\n## Spark vs. sklearn\n\nOur second objective of this project was to compare the two pipelines of Spark and sklearn - the distributed library and the python machine learning powerhouse. \n\nOn the surface they are quite similar (their imports even share names), but examination in greater detail reveals significant differences. \n\n#### Structure\n\nBoth sklearn (through pandas) and Spark use DataFrame structures for data storage and manipulation. In sklearn the entire DataFrame is used for training, but in Spark all features have to be packed into a single column - extracting each row of values into a vector. Spark therefore trains off of only one column of data, which contains multiple columns of data - coding this aspect is not complex, but requires extra thought.\n\n#### Scalability \n\nIn terms of scalability, theoretically, Spark has the clear advantage. Sklearn has fantastic performance if (and only if) the data fits into memory. Python and sklearn do non-distributed in-memory processing and with small data sets (megabytes, perhaps up to a few gigabytes) we are in fact better off using sklearn. Spark allows for a comprehensive set of tools specifically tailored towards big data - one framework to ingest, process, apply machine learning algorithms, query, and distribute computing power. With rapidly changing dataset, Spark also allows for streaming; machine learning in real time. \n\nPractically speaking, we did not experience a significant difference in training time between Spark and sklearn, though we expected to see vast improvements in performance with Spark. But as we will discuss below, it's hard to know what to associate with our model design and what to pin on shared clusters.\n\n#### Random Forest Algorithm\n\nIn terms of our algorithm selection - Random Forest - it's important to understand that under the hood, Spark and sklearn are slightly different in how they calculate feature importance. The documentation for sklearn states it is using “an optimized version of the CART algorithm”, and while it is not explicitly mentioned in the documentation, it has been inferred that Spark is using ID3 with CART. Examining these algorithms in detail is beyond the scope of this project, but it is useful to delineate briefly the small differences in how they each calculate feature importance, as it may affect model performance (especially at scale).\n\nFor each decision tree, Scikit-learn calculates a nodes importance using Gini Importance. The importance for each feature on a decision tree is then calculated as the importance of feature i and the importance of node j normalized to a value between 0 and 1 and divided by the sum of all feature importance values. The final feature importance, at the Random Forest level, is it’s average over all the trees. \n\nFor each decision tree, Spark calculates a feature’s importance by summing the gain, scaled by the number of samples passing through the node - the importance of feature i, the number of samples reaching node j, and the impurity of node j. To calculate the final feature importance at the Random Forest level, first the feature importance for each tree is normalized in relation to the tree, then feature importance values from each tree are summed and normalized.\n\n#### Practical Considerations\n\nIn implementing our algorithm in Databricks we faced some practical hurdles. The distributed framework added overhead; we faced this when running the PCA and linear regression took nearly as much time as running all our initial pipelines in sklearn. The PySpark pipelines did not function as smoothly for doing the same transformations as we did in sklearn - going through a whole looping process to create all the individual column transformations was a time-intensive process. In trying to implement Breiman encoding in PySpark we hit a roadblock -  attempting literal SQL joins 131 times for each categorical variable was just not feasible. This implementation can be seen in our demo data set, where the compute concerns were not nearly as high.\n\nThe choice of our evaluation metric, though logical, created an additional headache as calculating the median requires a sort and an additional pass through the data (which is not true with the mean). \n\nOne further differentiation between sklearn and Spark is the ability to visualize the data. In this aspect sklearn has the upper hand. With support for pandas and matplotlib we can visualize results, verify assumptions, and use scipy functions as part of the machine learning workflow. \n\n#### Final Thoughts\n\nOverall, Spark is an execution engine optimized to handle the workload of distributed data engineering, while sklearn is perhaps better suited to data science after your data is ingested, processed, and transformed. We found that Spark was not optimized for algorithm exploration - the API was not intuitive to compare individual model performance in the context of cross-validation. Ultimately, there is no reason not to incorporate both for different use cases inside the same environment.\n\nFor this type of data in the real world - insurance claims - it's likely the model would be retrained on a fairly regular basis, but not concurrently with the ingestion of new data. It's hard to imagine the predictions changing drastically with the addition of a new training example. However, the training time could be a factor. Training the model took upwards of 18 hours, which in this case might not be an issue as the prediction model might be more static than dynamic. Real time results would be necessary on the prediction side - not necessarily the training side.\n\nIn our experience, the use of Databricks was both a blessing and a curse. It was simple to set up the envirnment and infrastructure for distributed computing, but difficult to gauge actual performance and time complexity due to the shared nature of the compute clusters. Our algorithms took over 12 hours to run in both environments, but it is hard to know what to attribute to shared resources and what to attribute to model design."],"metadata":{}},{"cell_type":"markdown","source":["#### Time Comparison for Key Analysis Portions\n(times taken manually from this notebook)"],"metadata":{}},{"cell_type":"code","source":["# create data\nspark_onehot = 19.37\nsk_onehot = 54/60\n\nspark_pca_linear = 60*1.22\nspark_ridge = 2.54 * 60\nspark_lasso = 48.62\nspark_rf = 22.32\nsk_all = 1.51 * 60\n\nsk_cv = 12.48\nspark_cv = 1.71 * 24\n\nfig, ax = plt.subplots(figsize=(5,3))\n\ncats = ['Spark OHE', 'Sklean OHE']\nvals = [spark_onehot, sk_onehot]\n\nsns.barplot(x = cats, y = vals)\nax.set_title(\"Time for One Hot Encoding Mechanism\")\nax.set_ylabel(\"Time (minutes)\")\ndisplay(fig)\n\n\n"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(8,3))\ncats = ['Spark PCA+Linear', 'Spark Lasso', 'Spark Ridge', 'Spark RF', 'Sklean All Pipelines']\nvals = [spark_pca_linear, spark_lasso, spark_ridge, spark_rf, sk_all]\n\nsns.barplot(x = cats, y = vals)\nax.set_title(\"Time for Modeling Pipeline\")\nax.set_ylabel(\"Time (minutes)\")\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(5,3))\ncats = ['Spark Tuning', 'Sklearn Tuning']\nvals = [spark_cv, sk_cv]\nsns.barplot(x = cats, y = vals)\nax.set_title(\"Time for Model Hypterparameter Tuning\")\nax.set_ylabel(\"Time (hours)\")\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["# Application of Course Concepts"],"metadata":{}},{"cell_type":"markdown","source":["### Feature Engineering & Data Transformation\n\nFeature engineering and data transformation were course concepts that played a pivotal role in this project.  As is often understood in the field of data science, preparing the data for machine learning can and often should dominate the data scientist's time since the models we produce are only as good as the data we use to train and validate them.  Put pointedly, garbage in will result in garbage out.  \n\nThe AllState dataset presented a few challenges in this domain.  As we discussed, the data itself appeared to have been encoded to mask the meaning of the variables and values provided.  This eliminated any domain-specific feature engineering we would have investigated.  However, the dataset did include 116 categorical variables, or features, which required transformation in order to serve as inputs to the suite of regression algorithms we explored.  \n\nWe implemented two approaches to transforming these varaibles.  For use with the OLS, Ridge, and Lasso Regression algorithms, we applied a **one-hot encoding** (OHE) to transform the data.  OHE was specifically selected over a simple numeric label encoding to avoid any implicit reasoning by the algorithm that the numeric encoding implied an order amongst the values.  Using OHE to essentially create a new binary feature for each value of each categoricalvariable, we allowed each to serve as an indicator only of how that binary flag affected the outcome variable.  These binary features, held in a Dataframe as columns, were then **assembled as a vector** for machine learning with Spark (Spark, The Definitive Guide, Chapter 25, Preprocessing and Feature Engineering).\n\nFor the Random Forest Regression algorithm, we applied **Breiman's method** to transform the categorical variables.  To perform this transformation, we assign each unique value of a given feature to the median outcome of all observations with that unique value.  We selected to take the median instead of the mean because of our selected outcome measure, MAE. (Asychronous Session 12.8)"],"metadata":{}},{"cell_type":"markdown","source":["### Scalability\n\nWhile we discussed this concept in our conclusion, it also deserves a mention here. \n\nOur discussion of Spark vs. sklearn is essentially a discussion of scalability and time complexity, and is analagous to the \"machine learning at scale\" class as a whole. If the dataset is small enough to fit in memory on a local machine, and the model can be trained once, sklearn is a viable tool. If, however, the dataset is large, or growing, or streaming, and the computational power needed to access the data and derive any meaning from the data is greater than one machine can handle - distributed computing through Spark is the only solution. Spark can also handle smaller datasets, which makes it more versatile if less intuitive. In our case, the dataset was small enough to work with both frameworks and to compare the differences. However, we did not see the differences we expected to see - likely due to the use of Databricks and shared clusters.\n\nIn the course of this project, we were naive in our initial assumptions, namely that Spark wouldn't take as long as sklearn. However, we did not take into account the nature of shared clusters on Databricks. It is difficult to compare true performance given this factor - as discussed in our conclusion."],"metadata":{}},{"cell_type":"markdown","source":["### Tradeoffs in Model Selection\n\nIn machine learning, we are always looking for the model that will minimize error, reduce complexity, and generalize the best. In selecting a model, we automatically introduce bias: the difference between the predicted values and the true values. If our training data contains a lot of noise, our model will overfit the data due to high variance: the difference between the model's predicted value and the average model prediction. With high bias we are prone to 'underfit' the data (capturing none of the underlying pattern), and with high variance we are prone to 'overfit' the data (capturing noise along with the underlying pattern). Both render our model useless. A simple model with few parameters may have high bias and low variance, whereas a complex model with many parameters may have low bias and high variance. Therefore we must find a balance to neither overfit nor underfit our data - as a model cannot be both more complex and less complex simultaneously. In order to minimize both bias and variance, it is necessary to 'decompose' these sources of error to choose the best model. \n\nPractically speaking, bias-variance decomposition involves first bootstrapping the training data to simulate multiple training sets, and then fitting a model many times to the bootstrapped data to get an average model. Next, we would apply a learning algorithm to each replicated set of training data. In this case it is important to hold back some extra training data and compute the predicted values using this held back data. The bias and variance can then be plotted based on models differing in complexity to find the optimum balance.\n\nThe Random Forest algorithm minimizes bias and variance as it is bootstrapped by design.\n\nDecision Trees on their own have extremely low bias as they tend to overfit the training data. Each “prediction” made with one Decision Tree on our data set would be one entry in the \"loss\" column. This results in high variance and bad results on unseen data. While this individual tree would be overfit to the training data with large error, bagging (Bootstrap Aggregating) takes advantage of the fact that a high number of uncorrelated errors would average out to zero. Bagging creates multiple trees with random samples from the training data (with replacement). This is the \"Random Forest\", and the tradeoffs for model complexity - and the sources of bias and variance - come from hyperparameters like the number of trees, number of splits, number of features per tree, etc. \n\nHad we chosen linear regression instead of Random Forest, we would have further options to reduce bias and variance: regularization. This is discussed below in OLS assumptions.\n\nSources: https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229, https://people.eecs.berkeley.edu/~jrs/189/lec/12.pdf, https://towardsdatascience.com/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4, ILS book"],"metadata":{}},{"cell_type":"markdown","source":["### Model Assumptions\n\n#### Linear Models\nOrdinary least squares (OLS) regression is a very flexible method that involves a few assumptions. To perform linear regression, numerical data is needed, meaning numeric encodings are needed for any categorical fields. In our example, we have done that through one-hot encoding. At this stage, you can run the linear regression just fine but there are further assumptions needed to prove that the coefficients estimated by OLS are unbiased. There are 4 key assumptions in order to prove this:\n* The relationship between the input variables and output variables is actually linear in nature, with some disturbance. In other words, the data can actually be modeled with `y = b * x + b_0 + error`.\n  * For our data, when we train our machine learning models we assume this to be true. In reality, this is probably a generous assumption because there are likely some factors that have different types of effects. For instance, we could easily imagine the appropriate representation of the data to be a squared variable instead of the normal variable. If we don't explicily encode this into our inputs, we inherently violate the OLS assumptions.\n* We have random sampling of our data from the relevant population at large.\n  * Our team is skeptical that this is a truly random subset of data, given that Allstate had to clear this dataset to be usable to the public. There is likely some subset of claims that they were more inclined to expose. That being said, with a large sample size the effects of this are (hopefully) negligible.\n* The input variables each have some variance (not all the same value)\n  * All of our categorical variables have at least 2 categories in the data, and our numerical predictors have many different values. This assumption checks out with no problems.\n* No matter the value of our inputs, we should expect the random noise around the outcome variable to be the same.\n  * This can be tested by plotting out a simple scatterplot of our residuals against any one of our input variables. An alternative approach is to look at our actual vs predicted plot. Given that this isn't a perfect straight line, we can assess that it is highly unlikely that this assumption holds true.\n  \nSo, in short, we're pretty sure we are in violation of having unbiased estimates for our coefficients. This would definitely matter if the main purpose of our model was inference and understanding- however, with a dataset that is completely anonymized this is not so much a concern, because we don't know what any of our inputs really mean anyways. In a real world use case, we would really want to know the purposes of the model to asses how problematic this is.\n\nWhile not a core assumption of OLS, another item that can trip up a linear model are highly correlated input features. The estimates of the coefficients of highly correlated features will have high variance, but on aggregate will be close to the overall effect of those variables. In other words, the model knows that the correlated features have some degree of effect, but will be unsure specifically where to attribute the effects to each of them.\n\n_(Reference for OLS: Jeffrey Woodlridge, Introductory Econometrics, 4th Edition, Chapter 2)_\n\n#### Lasso + Ridge\n\nOne way to combat this last shortcoming of OLS, highly correlated features, is to use regularization (either L1 or L2) to reduce the effects of any given variable. L1 tends to shrink nonrelevant variables to have a coefficient of zero, whereas L2 reduces the overall magnitude of all of the coeficients to be closer to zero (but not necessarily any one to actual zero). The difference in behavior stems from the fact that lasso regression's regularization term is the sum of the absolute value of coefficients, whereas the ridge term is the sum of squared coefficients. While the equations generated from ridge regression (L2) and lasso regression (L1) look very similar in practice to OLS, the assumptions are slightly different. The input data for these algorithms needs to be standardized to the same relative scale. The rationale for this is that if the data is not scaled, the algorithms will naturally penalize inputs with different magnitudes not uniformly and some variables may dominate the regularization. By centering and scaling, the data are all on the same relative scales and we have a better apples-to-apples comparison where the penalties are applied uniformly. With our data being inherently bounded between 0 and 1 for continuous values, we were fine to proceed without doing further processing on this step.\n\n_(Reference for Lasso, Ridge: Hastie et all, The Elements of Statistical Learning, 2nd Edition, Section 3.4.1)_\n\n#### Tree-based algorithms\n\nAs an alternative approach, we could use decision trees or aggregations of trees (random forest or gradient boosted trees) in order to predict our outcome. These are nice because they have relatively few assumptions. They can work with categorical and real-valued data, although the treatment for each of these varies slightly in execution slightly. \n\nFor real-valued data, the algorithm will look to find test all possible split points in the input to find the one that reduces variance in the child nodes. Thus, all that is required that our data is able to be sorted to make this calculation. Note that no standardization or scaling is needed; this only depends on the order of the data, whatever that order is. The main situation where this can't be done is with missing values. However there is an easy fix- you can create a new column for whether the value is missing or not missing, and then fill in the missing values of the original data with a value. The tree algorithm now has an additional split decision variable it can make to ask if the knowledge that data is missing helps predict the outcome correctly.\n\nThe missing encoding is one example of how the tree deals with categorical features. In general, missingness isn't such a big deal for categorical variables, because for a categorical variable of `n` categories, we can simply encode missing values as the `n + 1` category. If a feature is treated as a nonordered category for the decision, we make binary decisions of one category vs. all others. In the sitation of a ranked category, we can encode these with the appropriate ranking and treat these as numeric. Note that you could alternatively one-hot encode the categorical variables into many different binary variables, as this boils down to making more or less the same comparison. Unlike other algorithms, this is not a necessary step and can hurt interpretability. In our data, we chose to do Breiman encoding, which encodes categorical variables with an aggregate statistic (medians in our case) per category. This gives the advantage of giving fewer optimal split points for comparison to the algorithm.\n\nNote that adding features potentially with something like one hot encoding makes more binary decisions required to get to the same effective decision. We can adjust how much these effects are present by manipulating tree depth or the number of features considered at each split node, but overall it's probably a better decision to not increase your dimensionality artificially. \n\nAdditionally, we can reduce the time-complexity to train a tree-based model by giving it less possible comparisons to make in the input variables. This could be done by any of the following:\n* Reducing the features that are fed into the model (either via dimensionality reduction like PCA, or feature selection like lasso)\n* Reducing the possible split points of our features (for instance, binning any real-valued inputs)\n\n_(Reference for Trees: Hastie et all, The Elements of Statistical Learning, 2nd Edition, Section 9.2)_\n\n_(Reference for effects of One Hot Encoding: https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)_"],"metadata":{}},{"cell_type":"markdown","source":["### Commutative and Associative Properties\n\nIn general, an operation (denoted as `op`) is commutative if the following holds true for some values `a` `b` and `c`:\n\n`a op b = b op a`\n\nAn operation is associative if the following holds true:\n\n`a op (b op c) = (a op b) op c`.\n\nIn words, this means that the commutative property says we don't need to worry about the order that we do an operation between two values, and the associative property means that we don't have to worry about how we group them. Addition and multiplication are both good examples of operations that have this property. These properties are useful when looking into mapreduce operations, since commutative and associative operations can be more effectively parallelized. If we can break down an operation into parts that have these properties and parts that do not, we can more effectively parallelize functions. For instance, a mean can be thought of as a sum component followed followed by a division. The sum can be parrallelized since it has these two properties, and then the division must happen at a single node. \n\nOne place where we expect that we saw there effects of this in our analysis was the conscious decision to use MAE as a metric rather than MSE. With MSE, we could have been replacing means instead of medians in most of our data. Our random forest implementation required the calculation of a lot of different medians which was a computationally intensive task, requiring a sort for each one. If we had chosen MSE, we could have been leveraging the fact that a sum is both communative and associative and can be done completely in a map step of mapreduce to improve our performance.\n\n\n_(Reference: Konwinski et all, Learning Spark, Chapter 6)_"],"metadata":{}},{"cell_type":"markdown","source":["_W261 Spring 2020 Final Project Team 1_\n\nJohn Boudreaux, Sarah Danzi, Alex West"],"metadata":{}}],"metadata":{"name":"W261_SP20_FINAL_PROJECT_TEAM1_AllState","notebookId":14766879041156},"nbformat":4,"nbformat_minor":0}
